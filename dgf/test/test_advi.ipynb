{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test ADVI\n",
    "\n",
    "It works!\n",
    "\n",
    "https://github.com/martiningram/jax_advi\n",
    "\n",
    "We modify the likelihood to a pseudolikelihood thats include the log det Jacobian of the transformation of prior $N(0,1)$.\n",
    "\n",
    "With better posterior covariances (like a MAP approximation): linear response variational Bayes. Operates on the result of a vanilla ADVI run.\n",
    "\n",
    "https://martiningram.github.io/vi-with-good-covariances/\n",
    "\n",
    "## Elliptical slice sampling\n",
    "\n",
    "Another technique which could make use of our $N(0,1)$ priors.\n",
    "Elliptical slice sampling is a MCMC method for problems with Gaussian priors. [Murray2010]\n",
    "For VI we use $N(0,1)$ priors which are then transformed and for nested sampling we use $U(0,1)$ priors which are then transformed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-18 11:47:33.116003: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "%run init.ipy\n",
    "from dgf import core\n",
    "from dgf import isokernels\n",
    "from lib import constants\n",
    "from dgf import bijectors\n",
    "from dgf.prior import lf\n",
    "from dgf.prior import source\n",
    "\n",
    "import tensorflow_probability.substrates.jax.distributions as tfd\n",
    "import tensorflow_probability.substrates.jax.bijectors as tfb\n",
    "\n",
    "import jax_advi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'dgf.prior.source' has no attribute '_get_source_params_ppf'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m PARAMS \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvar\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mT\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOq\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnoise_power\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      5\u001b[0m NUMPARAMS \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(PARAMS) \u001b[38;5;66;03m# == 5\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m source_params_ppf \u001b[38;5;241m=\u001b[39m \u001b[43msource\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_source_params_ppf\u001b[49m(\n\u001b[1;32m      8\u001b[0m     constants\u001b[38;5;241m.\u001b[39mSOURCE_BOUNDS, constants\u001b[38;5;241m.\u001b[39mSOURCE_MEDIAN, source\u001b[38;5;241m.\u001b[39mRHO\n\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# No correlations at all b/c synthetic prior\u001b[39;00m\n\u001b[1;32m     12\u001b[0m samples \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mhstack([\n\u001b[1;32m     13\u001b[0m     source_params_ppf(rand(\u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m1e4\u001b[39m), NUMPARAMS\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)),\n\u001b[1;32m     14\u001b[0m     source\u001b[38;5;241m.\u001b[39mnoise_power_ppf(rand(\u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m1e4\u001b[39m), \u001b[38;5;241m1\u001b[39m), constants\u001b[38;5;241m.\u001b[39mNOISE_FLOOR_DB)\n\u001b[1;32m     15\u001b[0m ])\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'dgf.prior.source' has no attribute '_get_source_params_ppf'"
     ]
    }
   ],
   "source": [
    "# Sample from the source and noise priors and fit a prior in the `z` domain\n",
    "BOUNDS = constants.SOURCE_BOUNDS.copy()\n",
    "BOUNDS['noise_power'] = [constants.NOISE_FLOOR_POWER, 1.]\n",
    "PARAMS = ['var', 'r', 'T', 'Oq', 'noise_power']\n",
    "NUMPARAMS = len(PARAMS) # == 5\n",
    "\n",
    "source_params_ppf = source._get_source_params_ppf(\n",
    "    constants.SOURCE_BOUNDS, constants.SOURCE_MEDIAN, source.RHO\n",
    ")\n",
    "\n",
    "# No correlations at all b/c synthetic prior\n",
    "samples = np.hstack([\n",
    "    source_params_ppf(rand(int(1e4), NUMPARAMS-1)),\n",
    "    source.noise_power_ppf(rand(int(1e4), 1), constants.NOISE_FLOOR_DB)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import corner\n",
    "\n",
    "fig = corner.corner(\n",
    "    samples,\n",
    "    labels=PARAMS,\n",
    "    show_titles=True,\n",
    "    smooth=.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.log(samples)\n",
    "z_mean = np.mean(z, axis=0)\n",
    "z_cov = np.cov(z.T)\n",
    "z_sigma = np.sqrt(np.diag(z_cov))\n",
    "z_corr = np.diag(1/z_sigma) @ z_cov @ np.diag(1/z_sigma)\n",
    "L_z_corr = np.linalg.cholesky(z_corr)\n",
    "\n",
    "z_bounds = np.log(np.array([\n",
    "    BOUNDS[k] for k in PARAMS\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dynesty\n",
    "import scipy.stats\n",
    "\n",
    "static_bijector = tfb.Chain([\n",
    "    tfb.Exp(), tfb.SoftClip(\n",
    "        z_bounds[:,0], z_bounds[:,1], z_sigma\n",
    "    )\n",
    "])\n",
    "\n",
    "def getprior(rescale):\n",
    "    L = np.diag(rescale*z_sigma) @ L_z_corr\n",
    "    prior = tfd.TransformedDistribution(\n",
    "        distribution=tfd.MultivariateNormalTriL(\n",
    "            loc=z_mean,\n",
    "            scale_tril=L\n",
    "        ),\n",
    "        bijector=static_bijector\n",
    "    )\n",
    "    return prior\n",
    "\n",
    "def loglike(rescale, data=samples):\n",
    "    prior = getprior(rescale)\n",
    "    lp = np.sum(prior.log_prob(data))\n",
    "    return -np.inf if np.isnan(lp) else float(lp)\n",
    "\n",
    "def ptform(\n",
    "    u,\n",
    "    rescale_prior=scipy.stats.expon(scale=1.)\n",
    "):\n",
    "    return rescale_prior.ppf(u)\n",
    "\n",
    "ndim = NUMPARAMS\n",
    "sampler = dynesty.NestedSampler(loglike, ptform, ndim, nlive=ndim*5)\n",
    "sampler.run_nested()\n",
    "results = sampler.results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rescale (stretch) factors\n",
    "results.samples[-1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Very good model of the data without problems at the edges\n",
    "priorml = getprior(results.samples[-1,:])\n",
    "\n",
    "fig = corner.corner(\n",
    "    np.array(priorml.sample(100000,seed=jax.random.PRNGKey(1387))),\n",
    "    labels=PARAMS,\n",
    "    show_titles=True,\n",
    "    smooth=.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prior_bijector(mean, cov, **kwargs):\n",
    "    color = bijectors.color_bijector(mean, cov)\n",
    "    return tfb.Chain([static_bijector, color])\n",
    "\n",
    "L = np.diag(results.samples[-1,:]*z_sigma) @ L_z_corr\n",
    "z_cov_ml = L @ L.T\n",
    "\n",
    "bijector = prior_bijector(z_mean, z_cov_ml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior = tfd.TransformedDistribution(\n",
    "    distribution=tfd.MultivariateNormalDiag(scale_diag=jnp.ones(NUMPARAMS)),\n",
    "    bijector=bijector,\n",
    "    name='Prior'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the convention: we need to **SUBTRACT** the forward log det jacobian\n",
    "\n",
    "# Using TransformedDistribution\n",
    "theta, theta_lp = prior.experimental_sample_and_log_prob(seed=jax.random.PRNGKey(10))\n",
    "display(theta, theta_lp)\n",
    "\n",
    "# Using bijector explicitly \n",
    "z = bijector.inverse(theta)\n",
    "z_lp = tfd.MultivariateNormalDiag(scale_diag=jnp.ones(NUMPARAMS)).log_prob(z)\n",
    "z_lp - bijector.forward_log_det_jacobian(z).squeeze(), theta_lp # Equal?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpack(theta):\n",
    "    a = jnp.vstack([theta['var'], theta['r'], theta['T'], theta['Oq'], theta['noise_power']])\n",
    "    return a.T\n",
    "\n",
    "def pack(a):\n",
    "    var, r, T, Oq, noise_power = a.T\n",
    "    return dict(var=var, r=r, T=T, Oq=Oq, noise_power=noise_power)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_samples = prior.sample(100000, seed=jax.random.PRNGKey(54544))\n",
    "\n",
    "import corner\n",
    "corner.corner(np.array(test_samples), labels=['var', 'r', 'T', 'Oq', 'noise_power'])\n",
    "\n",
    "pack(test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have annoying behavior of the density functions\n",
    "# near the boundaries (and nans if you get too close)\n",
    "# There *is* mass at the boundaries, as can be seen from\n",
    "# samples, but not overwhelmingly so\n",
    "MEDIANS = constants.SOURCE_MEDIAN.copy()\n",
    "MEDIANS['noise_power'] = constants.db_to_power(-30.)\n",
    "\n",
    "def probe_param_bounds(param, n=1000):\n",
    "    lower, upper = BOUNDS[param]\n",
    "    values = jnp.linspace(lower, upper, n)\n",
    "\n",
    "    a = unpack(MEDIANS)\n",
    "    a = np.repeat(a[None,:], n, axis=0)\n",
    "    theta_test = pack(a)\n",
    "    theta_test[param] = values\n",
    "    a = unpack(theta_test)\n",
    "    \n",
    "    return values, prior.log_prob(a)\n",
    "\n",
    "def test_param_bounds(param, n=1000):\n",
    "    values, lp = probe_param_bounds(param, n=n)\n",
    "    plot(values, lp)\n",
    "    title(param)\n",
    "    ylabel(f'log prior({param}|median values of other params)')\n",
    "    xlabel(param)\n",
    "    show()\n",
    "\n",
    "test_param_bounds('var')\n",
    "test_param_bounds('r')\n",
    "test_param_bounds('T')\n",
    "test_param_bounds('Oq')\n",
    "test_param_bounds('noise_power')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.scipy.stats import norm\n",
    "from jax.experimental.host_callback import call\n",
    "\n",
    "def minus_inf_if_nan(x):\n",
    "    return jax.lax.cond(jnp.isnan(x), lambda: -jnp.inf, lambda: x)\n",
    "\n",
    "def calculate_prior(packed_z): # Standardnormal\n",
    "    z = unpack(packed_z)\n",
    "    return jnp.sum(norm.logpdf(z))\n",
    "\n",
    "def calculate_likelihood(theta, sample, config):\n",
    "    R = core.kernelmatrix_root_gfd_oq(\n",
    "        config['kernel'],\n",
    "        theta['var'],\n",
    "        theta['r'],\n",
    "        sample['t'],\n",
    "        config['kernel_M'],\n",
    "        theta['T'],\n",
    "        theta['Oq'],\n",
    "        config['c'],\n",
    "        config['impose_null_integral']\n",
    "    )\n",
    "    logl = core.loglikelihood_hilbert(R, sample['u'], theta['noise_power'])\n",
    "    return logl\n",
    "\n",
    "def calculate_pseudo_likelihood(packed_z, sample, config):\n",
    "    \"\"\"\n",
    "    We perform a hack here and enforce the impact of the log det jacobian **of the prior**\n",
    "    by summing it with the likelihood. This is correct as log prior and log likelihood\n",
    "    are being summed to calculate the log posterior. But it smells a bit because\n",
    "    technically the log volume correction should only be applied to the prior, as the\n",
    "    likelihood is not a density with respect to the parameters, only to the data.\n",
    "    \"\"\"\n",
    "    z = unpack(packed_z)\n",
    "    \n",
    "    # Calculate the actual likelihood L(theta) = p(sample|theta)\n",
    "    theta = pack(bijector.forward(z))\n",
    "    log_like = calculate_likelihood(theta, sample, config).squeeze()\n",
    "    \n",
    "    # Doesn't work with gradients\n",
    "    #def printdebug(theta):\n",
    "        #print({k: float(v) for k, v in theta.items()})\n",
    "\n",
    "    #call(printdebug, theta)\n",
    "    \n",
    "    # Calculate the log volume factors of the transforms `z -> theta`\n",
    "    # Note the minus sign here!! This is the correct way.\n",
    "    prior_log_det_jac = -bijector.forward_log_det_jacobian(z).squeeze()\n",
    "    \n",
    "    return minus_inf_if_nan(log_like + prior_log_det_jac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lf_samples = source.get_lf_samples()\n",
    "\n",
    "z = randn(5)\n",
    "packed_z = pack(z)\n",
    "theta = pack(bijector.forward(z))\n",
    "\n",
    "sample = lf_samples[1]\n",
    "\n",
    "config = dict(\n",
    "    kernel_name = 'Matern32Kernel',\n",
    "    kernel_M = 128,\n",
    "    use_oq = True,\n",
    "    impose_null_integral = True\n",
    ")\n",
    "\n",
    "assert config['use_oq'] == True\n",
    "config['kernel'] = isokernels.resolve(config['kernel_name'])\n",
    "config['c'] = constants.BOUNDARY_FACTOR\n",
    "\n",
    "calculate_prior(packed_z), calculate_pseudo_likelihood(packed_z, sample, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "log_prior_func = jax.jit(calculate_prior)\n",
    "log_like_func = jax.jit(partial(calculate_pseudo_likelihood, sample=sample, config=config))\n",
    "\n",
    "packed_z = pack(randn(5))\n",
    "\n",
    "display(jax.value_and_grad(log_prior_func)(packed_z))\n",
    "display(jax.value_and_grad(log_like_func)(packed_z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We get infs in the objective function and nans in the gradients\n",
    "# when one of the `M` samples happens to hit a bound of the\n",
    "# `bijector`. If it\n",
    "# happens, all evaluations return infs and nans. So the problems\n",
    "# are due to the problematic behavior of the log det jac of\n",
    "# the prior transformation from N(0,I) to the actual model\n",
    "# parameters at the bounds\n",
    "# But our log likelihood is well behaved!!\n",
    "#\n",
    "# How to fix??\n",
    "# Either put `M = 3` very low or (and we did it here)\n",
    "# rescale the covariance `cov_z` to be much smaller such that\n",
    "# the bounds never get reached.\n",
    "import jax_advi.advi\n",
    "\n",
    "theta_shapes = {\n",
    "    'var': (),\n",
    "    'r': (),\n",
    "    'T': (),\n",
    "    'Oq': (),\n",
    "    'noise_power': ()\n",
    "}\n",
    "\n",
    "result = jax_advi.advi.optimize_advi_mean_field(\n",
    "    theta_shapes,\n",
    "    log_prior_func,\n",
    "    log_like_func,\n",
    "    verbose=True,\n",
    "    M=50,\n",
    "    #var_param_inits={'mean': (0.,1.), 'log_sd': (0.,1.)},\n",
    "    opt_method=\"L-BFGS-B\" # This is faster and seems to be leap succesfully over early local minima\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pack(bijector.forward(unpack(result['free_means'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior = pack(bijector.forward(unpack(result['draws'])))\n",
    "\n",
    "hist(np.array(posterior['T']))\n",
    "\n",
    "corner.corner(np.array(unpack(posterior)), labels=PARAMS, \n",
    "              #range=[BOUNDS[k] for k in PARAMS]\n",
    "             );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The open quotient `Oq` of the LF model correlates only moderately with the OQ as we see it. Namely we see the OQ as a \"hard\" close where the DGF waveform is zero. The LF model has an exponential return phase and a \"soft\" close such that the `Oq` is quite fuzzily defined. So we cannot expect our inferred `Oq` to correspond with the `Oq` of LF, because of this soft return phase. Our implementation of `Oq` is just dividing the pitch period into a hard zero (closed) phase and a nonzero (open) phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Try better errorbars from <https://github.com/martiningram/jax_advi/blob/main/examples/Tennis%20example.ipynb>\n",
    "\n",
    "from jax_advi.lrvb import compute_lrvb_covariance, get_posterior_draws_lrvb\n",
    "\n",
    "lrvb_free_sds, lrvb_cov_mat = compute_lrvb_covariance(\n",
    "    result['final_var_params_flat'], result['objective_fun'], result['shape_summary'], batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matshow(lrvb_cov_mat); colorbar();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrvb_free_sds # GOOD: These are several times larger than the vanilla free_sds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result['free_sds']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
